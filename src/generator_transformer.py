import os
import random
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from utils import weights_init, TransformerModel


class Gen_args(object):
    def __init__(self, 
                 vocab_size=3000, 
                 emb_dim=300, 
                 num_head=64,
                 hid_dim=64,
                 num_enc_l=0.5,
                 num_dec_l=64,
                 dropout=0.5,
                 out_dim=2):
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.num_head = num_head
        self.hid_dim = hid_dim
        self.num_enc_l = num_enc_l
        self.num_dec_l = num_dec_l
        self.dropout = dropout
        self.out_dim = out_dim

class Generator(nn.Module):
    '''
    Generator
    '''
    def __init__(self, G_args, use_cuda=False):
        super(Generator, self).__init__()
        self.args = G_args
        self.use_cuda = use_cuda
        # Encoder
        self.transform = TransformerModel(self.args.vocab_size, 
                                          self.args.emb_dim, 
                                          self.args.out_dim, 
                                          self.args.num_head, 
                                          self.args.hid_dim,
                                          self.args.num_enc_l,
                                          self.args.num_dec_l, 
                                          self.args.dropout)
        if self.use_cuda:
            self.transform = self.transform.cuda()
        self.apply(weights_init)

    def forward(self, input):
        """
        Args:
            x: (batch_size, seq_len, 2), sequence of tokens generated by generator
        """
        output, encoder = self.transform(input, has_mask=True, USE_CUDA=self.use_cuda)
        pred = F.log_softmax(output, dim=2)
        batch_size = pred.size(0)
        seq_len = pred.size(1)
        mask = input[:,:,1].float()
        padding = torch.cat([torch.zeros(batch_size, seq_len, self.args.vocab_size - 1), \
                             torch.ones(batch_size, seq_len, 1)], axis=2)
        if self.use_cuda:
            padding = padding.cuda()
        pred = pred * mask.unsqueeze(2) + padding * (1 - mask.unsqueeze(2))
        pred = pred.view(-1, self.args.vocab_size)
        return pred, encoder

        